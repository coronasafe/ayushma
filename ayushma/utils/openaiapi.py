from typing import List

import openai
import tiktoken
from django.conf import settings
from langchain.schema import AIMessage, HumanMessage
from pinecone import QueryResponse

from ayushma.models import Chat, ChatMessage, Project
from ayushma.utils.langchain import LangChainHelper


def get_embedding(
    text: List[str],
    model: str = "text-embedding-ada-002",
    openai_api_key: str = settings.OPENAI_API_KEY,
) -> List[List[float]]:
    """
    Generates embeddings for the given list of texts using the OpenAI API.

    Args:
        text (List[str]): A list of strings to be embedded.
        model (str, optional): The name of the OpenAI model to use for embedding.
            Defaults to "text-embedding-ada-002".

    Returns:
        A list of embeddings generated by the OpenAI API for the input texts.

    Raises:
        OpenAIError: If there was an error communicating with the OpenAI API.

    Example usage:
        >>> get_embedding(["Hello, world!", "How are you?"])
        [[-0.123, 0.456, 0.789, ...], [0.123, -0.456, 0.789, ...]]

    """
    openai.api_key = openai_api_key
    res = openai.Embedding.create(input=text, model=model)
    return [record["embedding"] for record in res["data"]]


def get_sanitized_reference(pinecone_references: List[QueryResponse]) -> str:
    """
    Extracts the text from the Pinecone QueryResponse object and sanitizes it.

    Args:
        pinecone_reference (List[QueryResponse]): The similar documents retrieved from
            the Pinecone index.

    Returns:
        A string containing the text from the Pinecone QueryResponse object.

    Example usage:
        >>> get_sanitized_reference([QueryResponse(...), QueryResponse(...)])
        "Hello, world! How are you?,I am fine. Thank you."
    """
    sanitized_reference = ""

    for reference in pinecone_references:
        for match in reference.matches:
            sanitized_reference += str(match.metadata["text"]).replace("\n", " ") + ","

    return sanitized_reference


def num_tokens_from_string(string: str, encoding_name: str) -> int:
    """Returns the number of tokens in a text string."""
    encoding = tiktoken.get_encoding(encoding_name)
    num_tokens = len(encoding.encode(string))
    return num_tokens


def split_text(text):
    """Returns one string split into n equal length strings"""
    n = len(str)
    number_of_chars = 8192
    parts = []

    for i in range(0, n, number_of_chars):
        part = text[i : i + number_of_chars]
        parts.append(part)

    return parts


def converse(text, openai_key, chat, match_number):
    top_k = match_number

    if not openai_key:
        raise Exception("OpenAI-Key header is required to create a chat or converse")
    openai.api_key = openai_key
    text = text.replace("\n", " ")
    nurse_query = ChatMessage.objects.create(message=text, chat=chat, messageType=1)
    num_tokens = num_tokens_from_string(text, "cl100k_base")
    embeddings: List[List[List[float]]] = []
    if num_tokens < 8192:
        try:
            embeddings.append(get_embedding(text=[text], openai_api_key=openai_key))
        except Exception as e:
            return Exception(
                e.__str__(),
            )
    else:
        parts = split_text(text)
        for part in parts:
            try:
                embeddings.append(get_embedding(text=[part], openai_api_key=openai_key))
            except Exception as e:
                raise Exception(
                    e.__str__(),
                )
    # find similar embeddings from pinecone index for each embedding
    pinecone_references: List[QueryResponse] = []

    for embedding in embeddings:
        try:
            similar: QueryResponse = settings.PINECONE_INDEX_INSTANCE.query(
                vector=embedding,
                top_k=top_k,
                namespace=str(chat.project.external_id),
                include_metadata=True,
            )
            pinecone_references.append(similar)
        except Exception as e:
            raise Exception(
                e.__str__(),
            )
    reference = get_sanitized_reference(pinecone_references=pinecone_references)
    lang_chain_helper = LangChainHelper(
        openai_api_key=openai_key, prompt_template=chat.project.prompt
    )
    # excluding the latest query since it is not a history
    previous_messages = (
        ChatMessage.objects.filter(chat=chat)
        .exclude(id=nurse_query.id)
        .order_by("created_at")
    )
    chat_history = []
    for message in previous_messages:
        if message.messageType == 1:  # type=USER
            chat_history.append(HumanMessage(content=f"Nurse: {message.message}"))
        elif message.messageType == 3:  # type=AYUSHMA
            chat_history.append(AIMessage(content=f"Ayushma: {message.message}"))
    response = lang_chain_helper.get_response(
        user_msg=text, reference=reference, chat_history=chat_history
    )
    response = response.replace("Ayushma: ", "")
    ChatMessage.objects.create(message=response, chat=chat, messageType=3)

    return response
